<!doctype html>
<html lang="en">
    <head>
        <!-- Required meta tags -->
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-172423174-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-172423174-1');
        </script>

        <a style="display:none" href="https://clustrmaps.com/site/1bcxg" title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=6gKmIxoHu5YyJujFCsmkix3_Ow00Hx5GVplWITVQ0SA&cl=ffffff"></a>
        <p style="display:none">
        <!-- Begin Web-Stat code v 7.0 -->
        <span id="wts2090845"></span>
        <script>var wts=document.createElement('script');wts.async=true;
        wts.src='https://app.ardalio.com/log7.js';document.head.appendChild(wts);
        wts.onload = function(){ wtslog7(2090845,1); };
        </script><noscript><a href="https://www.web-stat.com">
        <img src="https://app.ardalio.com/7/1/2090845.png" 
        alt="Web-Stat site statistics"></a></noscript>
        <!-- End Web-Stat code v 7.0 -->
        </p>

        <!-- Custom Fonts -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;1,100;1,200;1,300;1,400;1,500;1,600;1,700&display=swap" rel="stylesheet">

        <!-- Bootstrap CSS -->
        <link href="css/bootstrap.min.css" rel="stylesheet" integrity="sha384-+0n0xVW2eSR5OomGNYDnhzAbDsOXxcvSN1TPprVMTNDbiYZCxYbOOl7+AMvyTG2x" crossorigin="anonymous">

        <!-- Icons -->
        <script src="scripts/23e9e06de4.js"></script>
        <!-- <link rel="stylesheet" href="css/academicons.min.css"> -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

        <!-- Custom Stylesheet -->
        <link href="css/stylesheet.css" rel="stylesheet">

        <title>Arjun Ashok</title>
    </head>


    <body>
        <div class="container">

            <div class="px-4 row justify-content-md-center">
            <div class="col col-md-9">
            <nav class="navbar navbar-right navbar-expand-lg container navbar-light fixed-top bg-light">
                <div class="container-fluid">
                    <button class="navbar-toggler mx-auto" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                    </button>
                    <div class="collapse navbar-collapse" id="navbarNav">
                        <ul class="navbar-nav mx-auto nav-fill w-75">
                            <li class="nav-item">
                                <a class="nav-link" aria-current="page" href="#about">About</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="#news">News</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="#preprints">Latest Papers</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="#publications">Previous Work</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="#music">Music</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="#talks">Talks</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="#academic-service">Academic Service</a>
                            </li>
                        </ul>
                    </div>
                </div>
            </nav>
            </div>
            </div>


            <!-- About Me -->
            <div class="px-4 mt-5 row justify-content-md-center" id="about"> 
                <div class="col col-md-6 px-5">
                    <p align="justify">
                        <h1><b>Arjun</b> Ashok</h1>
                    </p>
                    <p align="justify">

                        <!-- <span style="color:darkred"> -->
                            I am a Visiting Researcher (Full-Time) at <a href="https://www.servicenow.com/research/" target="_blank">ServiceNow Research</a>, Montreal and a PhD student at 
<a href="https://mila.quebec/en/" target="_blank">MILA-Quebec AI Institute</a> and <a href="https://www.irina-lab.ai/about">CERC-AAI, Université de Montréal</a> advised by <a href="https://sites.google.com/view/irinarish/" target="_blank">Irina Rish</a> and <a href="https://www.alexdrouin.com/" target="_blank">Alexandre Drouin</a>. At ServiceNow, I also work closely with <a href="https://www.servicenow.com/research/author/etienne-marcotte.html" target="_blank">Étienne Marcotte</a>, <a href="https://www.servicenow.com/research/author/valentina-zantedeschi.html" target="_blank">Valentina Zantedeschi</a> and <a href="https://www.servicenow.com/research/author/nicolas-chapados.html" target="_blank">Nicolas Chapados</a>.
                        <!-- </span><br><br>  -->
                        My current research interests are in time series forecasting and decision-making, with a focus on designing scalable general-purpose models for
                        time series prediction tasks (forecasting, interpolation, imputation etc).

                    </p>

                    <p align="justify">
                        My email address is arjun.ashok [at] servicenow [dot] com. 
                        I'd love to connect and chat with you if we have shared interests, be it in <a href="https://ashok-arjun.github.io/#preprints" target="_blank">research</a> or <a href="https://ashok-arjun.github.io/#music" target="_blank">music</a> or anything else. Please shoot me an email if you'd like to connect!
                    </p>

                </div>
                <div class="col col-md-3">
                    <br><br>
                    <img src="images/Arjun_Ashok_Photo.png" style="max-width: 100%; height: auto; max-height: 210px;" class="img-fluid" alt="Descriptive Alt Text">
                </div>
                <div align="center" style="font-size: 24px;">
                    <a href="https://scholar.google.com/citations?hl=en&user=4ur98b4AAAAJ" target="_blank"><i class="ai ai-google-scholar contact-icon" aria-hidden="true"></i></a>
                    <a href="https://x.com/arjunashok37" target="_blank"><i class="fa fa-twitter contact-icon" aria-hidden="true"></i></a>
                    <a href="https://www.linkedin.com/in/arjun-ashok-psg/" target="_blank"><i class="fa fa-linkedin contact-icon" aria-hidden="true"></i></a>
                    <a href="https://github.com/ashok-arjun" target="_blank"><i class="fa fa-github contact-icon" aria-hidden="true"></i></a>
                    <a href="mailto:arjun.ashok.psg@gmail.com"><i class="fa fa-envelope contact-icon" aria-hidden="true"></i></a>
                    <!-- <a href="https://www.linkedin.com/in/arjun-ashok-31725a223/"><i class="fa fa-linkedin contact-icon" aria-hidden="true"></i></a> -->
                </div>
            </div>

            <!-- News -->
            <div class="px-4 py-2 row justify-content-md-center" id="news">
                <div class="col col-md-9 px-5">
                    <h2>News</h2>
                    <table class="table">
                        <tbody>

                            <tr>
                                <td><b>May&nbsp;'24</b></td>
                                <td>Presented TACTiS-2 at ICLR 2024. TACTiS-2 is A highly flexible model for multivariate probabilistic time series prediction tasks. Check out the tweet thread and poster <a href="https://x.com/arjunashok37/status/1787045322671210600" target="_blank">here</a>!
                                </td>
                            </tr>
                
                            <tr>
                                <td><b>Feb&nbsp;'24</b></td>
                                <td>The full version of <a href="https://time-series-foundation-models.github.io/lag-llama.pdf" target="_blank">Lag-Llama</a> released with open-source model checkpoints! Check the announcement <a href="https://twitter.com/arjunashok37/status/1755261111233114165" target="_blank">here</a>!
                                </td>
                            </tr>

                            <tr>
                                <td><b>Jan&nbsp;'24</b></td>
                                <td>I gave a talk on our efforts Towards General-Purpose Models for Time-Series Prediction at the <a href="https://www.meetup.com/montreal-time-series-meetup/events/298305168/">Winter 2024 Montreal Time Series Meetup</a>.
                                </td>
                            </tr>

                            <tr>
                                <td><b>Jan&nbsp;'24</b></td>
                                <td>TACTiS-2 accepted at ICLR 2024!
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <!-- Fill with old news -->
                    <div class="collapse" id="news_more">
                    <table class="table">
                        <tbody>


                            <tr>
                                <td><b>Dec&nbsp;'23</b></td>
                                <td>I gave a talk on Building Foundation Models for Time Series Data at the <a href="https://sites.google.com/mila.quebec/6thnslw-no/home?authuser=0" target="_blank">6th workshop on Neural Scaling Laws</a> co-located with NeurIPS 2023.
                                </td>
                            </tr>
                            
                            <tr>
                                <td><b>Oct&nbsp;'23</b></td>
                                <td>TACTiS-2 is out on arXiv.
                                </td>
                            </tr>

                            <tr>
                                <td><b>Oct&nbsp;'23</b></td>
                                <td>A preliminary version of Lag-Llama is out on arXiv.
                                </td>
                            </tr>

                            <tr>
                                <td><b>Jan '23</b></td>
                                <td>One <a href="https://arxiv.org/abs/2209.09858" target="_blank">paper</a> on out-of-distribution detection <span style="text-decoration: underline;">accepted</span> to ICLR 2023. This is work in collaboration with folks at <a href="https://mlcollective.org" target="_blank">ML Collective</a> mentored by <a href="https://rosanneliu.com" target="_blank">Rosanne Liu</a>.
                                </td>
                            </tr>
                            <tr>
                                <td><b>Jan '23</b></td>
                                <td>Started as a Visiting Researcher (Full-Time) at ServiceNow Research, Montreal. Excited to continue working on problems in time series representation learning!
                                </td>
                            </tr>
                            <tr>
                                <td class="col-sm-2"><b>Aug '22</b></td>
                                <td>Preliminary <a href="preprints/SSL_Weather_AAAISym22.pdf" target="_blank">work</a> on self-supervised learning objectives for weather time series <span style="text-decoration: underline;">accepted</span> at the <a href="https://sites.google.com/view/clvision2022/overview?authuser=0">AAAI 2022 Fall Symposium on Climate Change</a>. 
                                </td>
                            </tr>
                            <tr>
                                <td class="col-sm-2"><b>Jul '22</b></td>
                                <td>One <a href="papers/AshokECCV2022.pdf" target="_blank">paper</a> on Class-Incremental Learning <span style="text-decoration: underline;">accepted</span> as a full paper at <a href="https://eccv2022.ecva.net" target="_blank" style="text-decoration: none"><span>ECCV 2022</span></a>.</td>
                            </tr>
                            <tr>
                                <td><b>Jun '22</b></td>
                                <td>Started as a Research Intern at IBM Research, India. I'll be working on building self-supervised learning objectives and pre-trained models for geospatial weather time series.</td>
                            </tr>
                            <tr>
                                <td><b>Jun '22</b></td>
                                <td>One <a href="https://arxiv.org/abs/2204.07705" target="_blank">paper</a> on cross-task generalization in NLP <span style="text-decoration: underline;">submitted</span> to EMNLP 2022 (Update: Accepted). 
                                <!-- Check out our <a href="https://github.com/allenai/natural-instructions" target="_blank">repository</a> of language instructions for NLP tasks and the <a href="https://github.com/yizhongw/Tk-Instruct">code</a> for our experiments on cross-task generalization!</td> -->
                            </tr>
                            <tr>
                                <td class="col-sm-2"><b>Apr '22</b></td>
                                <td>One <a target="_blank" href="papers/AshokCLVISION.pdf">paper</a> on Class-Incremental Learning <span style="text-decoration: underline;">accepted</span> at the <a href="https://sites.google.com/view/clvision2022/overview?authuser=0">CLVISION Workshop</a> at CVPR 2022 as a non-archival paper (Update: Accepted at ECCV 2022).
                                </td>
                            </tr>
                            <tr>
                                <td><b>Apr '22</b></td>
                                <td>One <a target="_blank" href="papers/AshokRescienceC.pdf">reproducibility report</a> on Self-Supervision and Few-shot Learning <span style="text-decoration: underline;">accepted</span> at the <a href="https://openreview.net/group?id=ML_Reproducibility_Challenge/2021/Fall">ML Reproducibility Challenge 2021 (Fall Edition)</a> and published at <a href="http://rescience.github.io">ReScience-C</a>.
                                </td>
                            <!-- </tr>
                            <tr>
                                <td><b>Mar '22</b></td>
                                <td>Started as a Research Engineering Intern at <a href="http://dubverse.ai/">dubverse.ai</a>. Excited to work on problems on large language models, custom machine translation and lip-synchronization here.
                                </td>
                            </tr> -->
                            <tr>
                                <td><b>Oct '21</b></td>
                                <td>One <a href="https://ojs.aaai.org/index.php/AAAI/article/view/21589">paper</a> on out-of-distribution generalization accepted as AAAI 2022 as a student abstract.
                                </td>
                            </tr>
                            <tr>
                                <td><b>Jun '21</b></td>
                                <td>Started as a Research Assistant at IIT Hyderabad under Prof. Vineeth Balasubramanian.
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    </div>
                    <p>
                        <a class="btn btn-secondary" data-bs-toggle="collapse" href="#news_more" role="button" aria-expanded="false" aria-controls="news_more">
                            Show More
                        </a>
                    </p>
                </div>
            </div>

            <!-- Preprints -->
            <div class="px-4 py-2 row justify-content-md-center" id="preprints">
                <div class="col col-md-9 px-5">
                    <h2>Latest Papers</h2>
                    <div class="card mb-3">
                        <div class="row g-0">
                            <!-- <div class="col-md-4 text-center my-4 px-2">
                                <img src="images/papers/compositional.png" width="100%" height="100%">
                            </div> -->
                            <div class="col-md-12 my-4">
                                <center>
                                <div class="card-body">
                                    <h5 class="card-title">
                                        TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series
                                    </h5>
                                    <strong>Arjun Ashok</strong>,
                                    Étienne Marcotte, Valentina Zantedeschi, Nicolas Chapados, Alexandre Drouin
                                    <br>
                                    <a href="https://iclr.cc/Conferences/2024" target="_blank">ICLR 2024</a>
                                    <br>
                                    <p>                                        <a class="btn btn-outline-primary" data-bs-toggle="collapse" href="#tldr_tactis2" role="button" aria-expanded="false" aria-controls="tldr_tactis2">TL;DR</a>
                                        <a class="btn btn-outline-primary" data-bs-toggle="collapse" href="#abs_tactis2" role="button" aria-expanded="false" aria-controls="abs_tactis2">Abstract</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://arxiv.org/abs/2310.01327">arXiv</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://github.com/ServiceNow/TACTiS">Code</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://openreview.net/forum?id=xtOydkE1Ku">OpenReview</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://x.com/arjunashok37/status/1787045322671210600">Tweet</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="posters/TACTIS_2_ICLR_2024_Poster.pdf">Poster</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://www.servicenow.com/blogs/2024/tactis-2-time-series-prediction">Blog</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://www.youtube.com/watch?v=ES34R-iGGVc">15-min Video</a>
                                    </p>
                                    <div class="collapse" id="tldr_tactis2">
                                        <div class="card card-body" align="justify">
                                            A flexible model for multivariate probabilistic time series prediction, simplifying the training of attentional copulas, with state-of-the-art accuracy on diverse forecasting tasks, while supporting interpolation and learning from irregular data.
                                        </div>
                                    </div>
                                    <div class="collapse" id="abs_tactis2">
                                        <div class="card card-body" align="justify">
                                            We introduce a new model for multivariate probabilistic time series prediction, designed to flexibly address a range of tasks including forecasting, interpolation, and their combinations. Building on copula theory, we propose a simplified objective for the recently-introduced transformer-based attentional copulas (TACTiS), wherein the number of distributional parameters now scales linearly with the number of variables instead of factorially. The new objective requires the introduction of a training curriculum, which goes hand-in-hand with necessary changes to the original architecture. We show that the resulting model has significantly better training dynamics and achieves state-of-the-art performance across diverse real-world forecasting tasks, while maintaining the flexibility of prior work, such as seamless handling of unaligned and unevenly-sampled time series.
                                        </div>
                                    </div>
                                    
                                </div>
                                </center>
                            </div>
                        </div>
                    </div>



                    <div class="card mb-3">
                        <div class="row g-0">
                            <!-- <div class="col-md-4 text-center my-4 px-2">
                                <img src="images/papers/compositional.png" width="100%" height="100%">
                            </div> -->
                            <div class="col-md-12 my-4">
                                <center>
                                <div class="card-body">
                                    <h5 class="card-title">
                                        Lag-Llama: Towards Foundation Models for Time Series Forecasting
                                    </h5>
                                    Kashif Rasul*, <strong>Arjun Ashok</strong>*, Andrew Robert Williams, Hena Ghonia, Rishika Bhagwatkar, Arian Khorasani, Mohammad Javad Darvishi Bayazi, George Adamopoulos, Roland Riachi, Nadhir Hassen, Marin Biloš, Sahil Garg, Anderson Schneider, Nicolas Chapados, Alexandre Drouin, Valentina Zantedeschi, Yuriy Nevmyvaka, Irina Rish <br> (* Co-first authorship, equal contribution, order arbitrary)
                                    <br>
                                    <span style="text-decoration: underline;">Preprint</span>. Preliminary work presented at <a href="https://sites.google.com/view/r0-fomo" target="_blank">NeurIPS 2023 Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models</a>
                                    <p>
                                        <a class="btn btn-outline-primary" data-bs-toggle="collapse" href="#tldr_lagllama" role="button" aria-expanded="false" aria-controls="tldr_lagllama">TL;DR</a>
                                        <a class="btn btn-outline-primary" data-bs-toggle="collapse" href="#abs_lagllama" role="button" aria-expanded="false" aria-controls="abs_lagllama">Abstract</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://time-series-foundation-models.github.io/lag-llama.pdf">Paper</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://github.com/time-series-foundation-models/lag-llama">Code</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://huggingface.co/time-series-foundation-models/Lag-Llama">Weights</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://colab.research.google.com/drive/13HHKYL_HflHBKxDWycXgIUAHSeHRR5eo?usp=sharing">Demo</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://twitter.com/arjunashok37/status/1755261111233114165">Tweet</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://www.youtube.com/watch?v=Mf2FOzDPxck">15-min Video</a>
                                    </p>
                                    <div class="collapse" id="tldr_lagllama">
                                        <div class="card card-body" align="justify">
                                            A foundation model for probabilistic time series forecasting with strong zero-shot and few-shot capabilities
                                        </div>
                                    </div>
                                    <div class="collapse" id="abs_lagllama">
                                        <div class="card card-body" align="justify">
                                        Over the past years, foundation models have caused a paradigm shift in machine learning due to their unprecedented capabilities for zero-shot and few-shot generalization. However, despite the success of foundation models in modalities such as natural language processing and computer vision, the development of foundation models for time series forecasting has lagged behind. We present Lag-Llama, a general-purpose foundation model for univariate probabilistic time series forecasting based on a decoder-only transformer architecture that uses lags as covariates. Lag-Llama is pretrained on a large corpus of diverse time series data from several domains, and demonstrates strong zero-shot generalization capabilities compared to a wide range of forecasting models on downstream datasets across domains. Moreover, when fine-tuned on relatively small fractions of such previously unseen datasets, Lag-Llama achieves state-of-the-art performance, outperforming prior deep learning approaches, emerging as the best general-purpose model on average. Lag-Llama serves as a strong contender to the current state-of-art in time series forecasting and paves the way for future advancements in foundation models tailored to time series data.
                                        </div>
                                    </div>
                                    
                                </div>
                                </center>
                            </div>
                        </div>
                    </div>





                </div>
            </div>


            <!-- Research -->
            <div class="px-4 py-2 row justify-content-md-center" id="publications">
                <div class="col col-md-9 px-5">
                    <h2>Previous Work</h2>
                    I previously worked on problems in out-of-distribution generalization, continual learning, and few-shot learning, spanning the domains of computer vision and natural language processing.
                    
                    <!-- <h4>Conference Publications</h4>
                    <h4>Publications as First-Author</h4> -->
                    <div class="card mb-3">
                        <div class="row g-0">
                            <!-- <div class="col-md-4 text-center my-4 px-2">
                                <img src="images/papers/compositional.png" width="100%" height="100%">
                            </div> -->
                            <div class="col-md-12 my-4">
                                <center>
                                <div class="card-body">
                                    <h5 class="card-title">
                                        Class-Incremental Learning with Cross-Space Clustering and Controlled Transfer
                                    </h5>
                                    <strong>Arjun Ashok</strong>,
                                    K J Joseph,
                                    Vineeth Balasubramanian
                                    <br>
                                    <span style="text-decoration: underline;">Accepted</span> at <a href="https://eccv2022.ecva.net">ECCV 2022</a>
<!--                                     <br>Also appeared at the CLVISION Workshop, CVPR 2022 -->
                                    <br>
                                    <p>
                                        <a class="btn btn-outline-primary" data-bs-toggle="collapse" href="#tldr_eccv2022" role="button" aria-expanded="false" aria-controls="tldr_eccv2022">TL;DR</a>
                                        <a class="btn btn-outline-primary" data-bs-toggle="collapse" href="#abs_eccv2022" role="button" aria-expanded="false" aria-controls="abs_eccv2022">Abstract</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="papers/AshokECCV2022.pdf">Paper</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://arxiv.org/abs/2208.03767">arXiv</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="http://cscct.github.io">Project Page</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://github.com/ashok-arjun/CSCCT">Code</a>
                                    </p>
                                    <div class="collapse" id="tldr_eccv2022">
                                        <div class="card card-body" align="justify">
                                            We propose two distillation-based objectives for class incremental learning that leverage the structure of the feature space to maintain accuracy on previous classes, as well as enable learning the new classes
                                        </div>
                                    </div>
                                    <div class="collapse" id="abs_eccv2022">
                                        <div class="card card-body" align="justify">
                                            In class-incremental learning, the model is expected to learn new classes continually while maintaining knowledge on previous classes. The challenge here lies in preserving the model's ability to effectively represent prior classes in the feature space, while adapting it to represent incoming new classes. We propose two distillation-based objectives for class incremental learning that leverage the structure of the feature space to maintain accuracy on previous classes, as well as enable learning the new classes. In our first objective, termed cross-space clustering (CSC), we propose to use the feature space structure of the previous model to characterize directions of optimization that maximally preserve the class - directions that all instances of a specific class should collectively optimize towards, and those that they should collectively optimize away from. Apart from minimizing forgetting, this indirectly encourages the model to cluster all instances of a class in the current feature space, and gives rise to a sense of herd-immunity, allowing all samples of a class to jointly combat the model from forgetting the class. Our second objective termed controlled transfer (CT) tackles incremental learning from an understudied perspective of inter-class transfer. CT explicitly approximates and conditions the current model on the semantic similarities between incrementally arriving classes and prior classes. This allows the model to learn classes in such a way that it maximizes positive forward transfer from similar prior classes, thus increasing plasticity, and minimizes negative backward transfer on dissimilar prior classes, whereby strengthening stability. We perform extensive experiments on two benchmark datasets, adding our method (CSCCT) on top of three prominent class-incremental learning methods. We observe consistent performance improvement on a variety of experimental settings.

                                        </div>
                                    </div>
                                    
                                </div>
                                </center>
                            </div>
                        </div>
                    </div>

                    <!-- <h4>Publications as Co-Author</h4> -->




                    <div class="card mb-3">
                        <div class="row g-0">
                            <!-- <div class="col-md-4 text-center my-4 px-2">
                                <img src="images/papers/compositional.png" width="100%" height="100%">
                            </div> -->
                            <div class="col-md-12 my-4">
                                <center>
                                <div class="card-body">
                                    <h5 class="card-title">
                                        Extremely Simple Activation Shaping for Out-of-Distribution Detection
                                    </h5>
                                    Andrija Djurisic,
                                    Nebojsa Bozanic,
                                    <strong>Arjun Ashok</strong>,
                                    <a href="http://rosanneliu.com/">Rosanne Liu</a>
                                    <br>
                                    <!-- Preprint to be out soon,  -->
                                    <span style="text-decoration: underline;">Accepted</span> at <a href="https://iclr.cc/Conferences/2023">ICLR 2023</a>
                                    <br>
                                    <p>
                                        <a class="btn btn-outline-primary" data-bs-toggle="collapse" href="#tldrASH" role="button" aria-expanded="false" aria-controls="tldrASH">TL;DR</a>
                                        <a class="btn btn-outline-primary" data-bs-toggle="collapse" href="#absASH" role="button" aria-expanded="false" aria-controls="absASH">Abstract</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://arxiv.org/abs/2209.09858">arXiv</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://andrijazz.github.io/ash/">Project Page</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://github.com/andrijazz/ash">Code</a>
                                    </p>
                                    <div class="collapse" id="tldrASH">
                                        <div class="card card-body" align="justify">
                                            We develop an extremely simple, post hoc, on-the-fly, and plug-and-play activation shaping method for out-of-distribution detection. 
                                        </div>
                                    </div>
                                    <div class="collapse" id="absASH">
                                        <div class="card card-body" align="justify">
                                            The separation between training and deployment of machine learning models implies that not all scenarios encountered in deployment can be anticipated during training, and therefore relying solely on advancements in training has its limits. Out-of-distribution (OOD) detection is an important area that stress-tests a model's ability to handle unseen situations: Do models know when they don't know? Existing OOD detection methods either incur extra training steps, additional data or make nontrivial modifications to the trained network. In contrast, in this work, we propose an extremely simple, post-hoc, on-the-fly activation shaping method, ASH, where a large portion (e.g. 90%) of a sample's activation at a late layer is removed, and the rest (e.g. 10%) simplified or lightly adjusted. The shaping is applied at inference time, and does not require any statistics calculated from training data. Experiments show that such a simple treatment enhances in-distribution and out-of-distribution sample distinction so as to allow state-of-the-art OOD detection on ImageNet, and does not noticeably deteriorate the in-distribution accuracy. We release alongside the paper two calls for explanation and validation, believing the collective power to further validate and understand the discovery.
                                        </div>
                                    </div>
                                    
                                </div>
                                </center>
                            </div>
                        </div>
                    </div>

                    <div class="card mb-3">
                        <div class="row g-0">
                            <!-- <div class="col-md-4 text-center my-4 px-2">
                                <img src="images/papers/compositional.png" width="100%" height="100%">
                            </div> -->
                            <div class="col-md-12 my-4">
                                <center>
                                <div class="card-body">
                                    <h5 class="card-title">
                                        Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks
                                    </h5>
                                    Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, 
                                    <strong>Arjun Ashok</strong>,
                                    ...,
                                    Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, Daniel Khashabi <b>(40 authors)</b>
                                    <br>
                                    <span style="text-decoration: underline;">Accepted</span> at <a href="https://2022.emnlp.org">EMNLP 2022</a>
                                    <br>
                                    <p>
                                        <a class="btn btn-outline-primary" data-bs-toggle="collapse" href="#tldr_emnlp" role="button" aria-expanded="false" aria-controls="tldr_emnlp">TL;DR</a>
                                        <a class="btn btn-outline-primary" data-bs-toggle="collapse" href="#abs1" role="button" aria-expanded="false" aria-controls="abs1">Abstract</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://arxiv.org/abs/2204.07705">arXiv</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://github.com/allenai/natural-instructions">Dataset</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://github.com/yizhongw/Tk-Instruct">Code</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://instructions.apps.allenai.org/">Project Page</a>
                                    </p>
                                    <div class="collapse" id="tldr_emnlp">
                                        <div class="card card-body" align="justify">
                                            We introduce a benchmark of 1,600+ diverse language tasks and their expert-written instructions, and rigorously benchmark cross-task/unseen-task generalization of models. We introduce Tk-Instruct, an encoder-decoder Transformer that is trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples) which outperforms existing larger models on our benchmark.
                                        </div>
                                    </div>
                                    <div class="collapse" id="abs1">
                                        <div class="card card-body" align="justify">
                                            How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions -- training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.
                                        </div>
                                    </div>
                                    
                                </div>
                                </center>
                            </div>
                        </div>
                    </div>


                    <!-- <h4>Workshop/Symposium Publications</h4> -->
                    <div class="card mb-3">
                        <div class="row g-0">
                            <!-- <div class="col-md-4 text-center my-4 px-2">
                                <img src="images/papers/compositional.png" width="100%" height="100%">
                            </div> -->
                            <div class="col-md-12 my-4">
                                <center>
                                <div class="card-body">
                                    <h5 class="card-title">
                                        Self-Supervised Representations of Geolocated Weather Time Series - an Evaluation and Analysis
                                    </h5>
                                    <strong>Arjun Ashok</strong>,
                                    Devyani Lambhate, Jitendra Singh
                                    <br>
                                    <span style="text-decoration: underline;">Accepted</span> at <a href="https://www.climatechange.ai/events/aaaifss2022">AAAI 2022 Climate Change Symposium</a> 
                                    <br>
                                    <p>
                                        <a class="btn btn-outline-primary" data-bs-toggle="collapse" href="#tldr_aaaiclim" role="button" aria-expanded="false" aria-controls="tldr_aaaiclim">TL;DR</a>
                                        <a class="btn btn-outline-primary" data-bs-toggle="collapse" href="#abs_aaaiclim" role="button" aria-expanded="false" aria-controls="abs_aaaiclim">Abstract</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="preprints/SSL_Weather_AAAISym22.pdf">Preprint</a>
                                        <!-- (Long Version coming soon!) -->
                                    </p>
                                    <div class="collapse" id="tldr_aaaiclim">
                                        <div class="card card-body" align="justify">
                                            We analyse existing self-supervised multivariate time series learning algorithms on their ability to learn representations of weather features, evaluating them on weather-driven downstream applications
                                        </div>
                                    </div>
                                    <div class="collapse" id="abs_aaaiclim">
                                        <div class="card card-body" align="justify">
                                            Self-supervised learning (SSL) algorithms are gaining traction in various domains as a general paradigm of learning representations from data, largely outperforming supervised learning algorithms in tasks where labelled data is limited and costly to collect. 
                                            In this work, we analyse existing self-supervised multivariate time series learning algorithms on their ability to learn representations of weather features, evaluating them on weather-driven downstream applications involving regression, classification and forecasting tasks.
                                            We experiment with a two-step protocol.
                                            In the first step, we employ an SSL algorithm and learn generic weather representations from multivariate weather data. 
                                            Then, in the next step, we use these representations and train simple linear models for multiple downstream tasks.
                                            Through our experiments on air quality prediction tasks, we highlight the benefits of self-supervised weather representations. The benefits include improved performance across multiple tasks, the ability to generalize with limited in-task data, and a reduction in training time and carbon emissions.
                                            We highlight several areas of future work and the potential impact that such algorithms can have on real-world problems.
                                            We expect such a direction to be relevant in multiple weather-driven applications supporting climate change mitigation and adaptation efforts.
                                        </div>
                                    </div>
                                    
                                </div>
                                </center>
                            </div>
                        </div>
                    </div>

                    <div class="card mb-3">
                        <div class="row g-0">
                            <!-- <div class="col-md-4 text-center my-4 px-2">
                                <img src="images/papers/compositional.png" width="100%" height="100%">
                            </div> -->
                            <div class="col-md-12 my-4">
                                <center>
                                <div class="card-body">
                                    <h5 class="card-title">
                                        Learning Modular Structures That Generalize Out-Of-Distribution
                                    </h5>
                                    <strong>Arjun Ashok</strong>,
                                    Chaitanya TD,
                                    Vineeth Balasubramanian
                                    <br>
                                    <span style="text-decoration: underline;">Accepted</span> at <a href="https://aaai.org/Conferences/AAAI-22/">AAAI 2022</a> Student Track
                                    <br>
                                    <p>
                                        <a class="btn btn-outline-primary" data-bs-toggle="collapse" href="#tldr1" role="button" aria-expanded="false" aria-controls="tldr1">TL;DR</a>
                                        <a class="btn btn-outline-primary" data-bs-toggle="collapse" href="#abs_aaai" role="button" aria-expanded="false" aria-controls="abs_aaai">Abstract</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="papers/AAAI22Modular.pdf">Short Version</a>
                                        <!-- (Long Version coming soon!) -->
                                    </p>
                                    <div class="collapse" id="tldr1">
                                        <div class="card card-body" align="justify">
                                            Designed two regularizers that enforce a network to preserve expert features that are reusable across domains, enabling them to extrapolate to unseen distributions better 
                                        </div>
                                    </div>
                                    <div class="collapse" id="abs_aaai">
                                        <div class="card card-body" align="justify">
                                            Out-of-distribution (O.O.D.) generalization remains to be a key challenge for real-world machine learning 
                                            systems. We describe a method for O.O.D. generalization that, through training, encourages models to only 
                                            preserve features in the network that are well reused across multiple training domains. Our method 
                                            combines two complementary neuron-level regularizers with a probabilistic differentiable binary mask 
                                            over the network, to extract a modular sub-network that achieves better O.O.D. performance than the 
                                            original network. Preliminary evaluation on two benchmark datasets corroborates the promise of our method.
                                        </div>
                                    </div>
                                    
                                </div>
                                </center>
                            </div>
                        </div>
                    </div>


                    <div class="card mb-3">
                        <div class="row g-0">
                            <!-- <div class="col-md-4 text-center my-4">
                                <embed src="images/papers/conv4-screenshot.png" width="100%">
                            </div> -->
                            <div class="col-md-12 my-4">
                                <center>
                                <div class="card-body">
                                    <h5 class="card-title">
                                        Does Self-Supervision Always Improve Few-Shot Learning?
                                    </h5>
                                    <strong>Arjun Ashok</strong>,
                                    Haswanth Aekula
                                    <br>
                                    <span style="text-decoration: underline;">Accepted</span> at <a href="http://rescience.github.io/">
                                        ReScience-C Journal</a> through the Machine Learning Reproduciblity Challenge (<a href="https://paperswithcode.com/rc2021">MLRC</a>) 2021
                                    <br>
                                    To be presented at the <a href="https://blog.neurips.cc/2022/08/15/journal-showcase/" taret="_blank">Journal Showcase Poster Session</a> at <a href="https://neurips.cc" target="_blank">NeurIPS 2022</a>
                                    <p>
                                        <a class="btn btn-outline-primary" data-bs-toggle="collapse" href="#tldr2" role="button" aria-expanded="false" aria-controls="tldr2">TL;DR</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="papers/AshokRescienceC.pdf">PDF</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://wandb.ai/meta-learners/fsl-ssl/reports/When-Does-Self-Supervision-Improve-Few-Shot-Learning---Vmlldzo5MDA5NzA?galleryTag=">W&B Blog</a>
                                        <a class="btn btn-outline-primary" target="_blank" href="https://github.com/ashok-arjun/fsl_ssl_working">Code</a>
                                    </p>
                                    <div class="collapse" id="tldr2">
                                        <div class="card card-body" align="justify">
                                            In contrast to prior literature, we show that the effectiveness of self-supervision in improving few-shot learning 
                                            highly depends on the architecture and image size used, and that using self-supervised to train models decreases cross-domain few-shot
                                            performance
                                        </div>
                                    </div>
                                </div>
                                </center>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Music -->
            <div class="px-4 py-2 row justify-content-md-center" id="music">
                <div class="col col-md-9 px-5">
                    <h2>Music</h2>
                    <table class="table">
                        <tbody>
                            <tr>
                                <div class="image_container" width="33%">
                                <td>I am a <a href="https://en.wikipedia.org/wiki/Carnatic_music">Carnatic</a> Vocalist and a student of Vidwan <a href="https://en.wikipedia.org/wiki/Bharat_Sundar" target="_blank">Bharat Sundar</a>. I have performed Carnatic concerts in multiple venues in India, and continue to perform in and around Montréal regularly. 
                                    </td>
                                </div>
                                <div class="image_container" width="33%">
                                <td><img width="100%" src="images/carnatic/may2022.jpeg"><br><center><small>May 2022</small></center></td>
                                </div>
                                <div class="image_container" width="33%">
                                <td><img width="100%" src="images/carnatic/jan2019.jpeg"><br><center><small>Jan 2019</small></center></td>
                                </div>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>

            <div class="px-4 py-2 row justify-content-md-center" id="talks">
                <div class="col col-md-9 px-5">
                    <h2>Talks</h2>
                
                    <ul>
                        <li><b>Towards General-Purpose Models for Time-Series Prediction</b>. The <a href="https://www.meetup.com/montreal-time-series-meetup/events/298305168/">Winter 2024 Montreal Time Series Meetup</a>, Montreal, Jan 2024.
                        <li><b>Building Foundation Models for Time Series Data</b>. <a href="https://sites.google.com/mila.quebec/6thnslw-no/home?authuser=0" target="_blank">The 6th workshop on Neural Scaling Laws</a> co-located with NeurIPS, Dec 2023.</li>
                    </ul>
                </div>
            </div>


            <div class="px-4 py-2 row justify-content-md-center" id="academic-service">
                <div class="col col-md-9 px-5">
                    <h2>Academic Service</h2>
                
                    <ul>
                        <li>Served as a reviewer at several conferences and journals: ICLR 2024, NeurIPS 2023, AISTATS 2022, AISTATS 2021, CVPR 2022, CVPR 2021, TMLR, TPAMI.</li>
                        <li>Organized the ICLR 2024 Time Series Meetup in Vienna in May 2024.</li>
                    </ul>
                </div>
            </div>


        </div>

        <!-- Footer -->
        <footer class="footer mt-auto py-3 bg-light">
            <div class="container text-center">
                <span class="text-muted">&copy; 2024 Arjun Ashok</span>
            </div>
        </footer>

        <!-- Bootstrap Bundle with Popper -->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-gtEjrD/SeCtmISkJkNUaaKMoLD0//ElJ19smozuHV6z3Iehds+3Ulb9Bn9Plx0x4" crossorigin="anonymous"></script>
    </body>
</html>
